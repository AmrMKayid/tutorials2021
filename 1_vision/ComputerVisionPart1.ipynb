{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ComputerVisionPart1_start.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8b92640dbf054e0d902192041a033974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e8a4c38d7c0d415ab97966a1fff2f2dc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2acfedd077914ebe81a2eb145ce05b45",
              "IPY_MODEL_ef318437ff8649f183b82ddaade32f00"
            ]
          }
        },
        "e8a4c38d7c0d415ab97966a1fff2f2dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2acfedd077914ebe81a2eb145ce05b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_56f1dfb0956a46de9dce6920f10cad1b",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_732c094501e24c2bbc93a3a816b21b37"
          }
        },
        "ef318437ff8649f183b82ddaade32f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8a6f080ed8144550883a54963f73ee75",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:00&lt;00:00,  8.26 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5d6c887da8b44cda0e1a3badb29e9de"
          }
        },
        "56f1dfb0956a46de9dce6920f10cad1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "732c094501e24c2bbc93a3a816b21b37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a6f080ed8144550883a54963f73ee75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5d6c887da8b44cda0e1a3badb29e9de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi1dtiTtK1Ov"
      },
      "source": [
        "## M2L: ConvNets and Computer Vision Tutorial (PART I)\n",
        "\n",
        "### Supervised classification, overfitting and inductive biases in convnets, and how to improve models through self-supervision\n",
        "### by Marco Buzzelli, Luigi Celona, Flavio Piccoli, and Simone Zini\n",
        "\n",
        "* Exercise 1: Implement and train a ResNetSE-18 classifier using supervised learning; enable/disable batch norm updates to see the effect.\n",
        "* Exercise 2: Inductive biases in convnets; comparison with MLP.\n",
        "* Exercise 3: Overfitting and regularization using weight decay.\n",
        "* Exercise 4: Enable self-supervised learning using data augmentation.\n",
        "\n",
        "**Questions**: \n",
        "1. What happens with resnet's performance when batch norm statistics are not updated? How about MLP? Why is one affected less than the other?\n",
        "2. What is resnet's train loss on permuted MNIST? How about the test accuracy? How is the MLP affected by the permutation?\n",
        "3. What other types of regularization could you use to avoid overfitting?\n",
        "4. In what applications do you expect the auxiliary self-supervised task to help more?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmGVZkJUkmpS"
      },
      "source": [
        "## Import JAX, Haiku, and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehm8gXYs18DX"
      },
      "source": [
        "!pip install git+https://github.com/deepmind/dm-haiku &> /dev/null\n",
        "!pip install -U tensorboard &> /dev/null\n",
        "!pip install -q dm-tree &> /dev/null\n",
        "!pip install git+https://github.com/deepmind/optax.git &> /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ho_PioVRL_o",
        "outputId": "660b2c03-de1d-4229-836b-60dc87efa66c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import tree\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "import warnings\n",
        "\n",
        "# Dataset libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Plotting library.\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "#@title Imports\n",
        "from typing import Iterable, Mapping, Tuple, \\\n",
        "    Generator, Sequence, Optional, Union\n",
        "\n",
        "# Don't forget to select GPU runtime environment in Runtime -> Change runtime type\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Define useful types\n",
        "Batch = Mapping[np.ndarray, np.ndarray]\n",
        "OptState = Tuple[optax.TraceState,\n",
        "                 optax.ScaleByScheduleState,\n",
        "                 optax.ScaleState]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P65vVFkUbTu"
      },
      "source": [
        "## Download dataset to be used for training and testing\n",
        "* MNIST images\n",
        "\n",
        "* 70000 28x28 grayscale images in 10 classes: digits from 0 to 9\n",
        "\n",
        "* train: 60000; test: 10000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEEkhc5KRa8t",
        "outputId": "1c977a04-c8f6-4c10-d2af-b53cf43dfc59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "8b92640dbf054e0d902192041a033974",
            "e8a4c38d7c0d415ab97966a1fff2f2dc",
            "2acfedd077914ebe81a2eb145ce05b45",
            "ef318437ff8649f183b82ddaade32f00",
            "56f1dfb0956a46de9dce6920f10cad1b",
            "732c094501e24c2bbc93a3a816b21b37",
            "8a6f080ed8144550883a54963f73ee75",
            "c5d6c887da8b44cda0e1a3badb29e9de"
          ]
        }
      },
      "source": [
        "# Dataset constants for MNIST dataset:\n",
        "# it contains low-res natural images (28x28x1) belonging to 10 classes.\n",
        "dataset_name = 'mnist'\n",
        "class_mnist = [u'0', u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9']\n",
        "train_split = 'train'\n",
        "eval_split = 'test'\n",
        "num_examples = {train_split: 60000,\n",
        "                eval_split: 10000}\n",
        "num_classes = 10\n",
        "\n",
        "train_ds = tfds.load(dataset_name, split=train_split)\n",
        "test_ds = tfds.load(dataset_name, split=eval_split)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b92640dbf054e0d902192041a033974",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptioâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMPjp0UU4Mx"
      },
      "source": [
        "## Display the images\n",
        "The gallery function below shows sample images from the data, together with their labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy0BWFwFUQ0J"
      },
      "source": [
        "MAX_IMAGES = 10\n",
        "\n",
        "\n",
        "def gallery(dataset, num_frames, title='Input images'):\n",
        "    num_frames = min(num_frames, MAX_IMAGES)\n",
        "    ff, axes = plt.subplots(1, num_frames,\n",
        "                            figsize=(num_frames, 1),\n",
        "                            subplot_kw={'xticks': [], 'yticks': []})\n",
        "    i = 0\n",
        "    sample = dataset.next()\n",
        "    while sample and i < num_frames:\n",
        "        image = sample['image']\n",
        "        if image.shape[-1] == 3:\n",
        "            axes[i].imshow(image)\n",
        "        else:\n",
        "            axes[i].imshow(image[:,:,0], cmap='gray')\n",
        "        axes[i].set_title(class_mnist[sample['label']])\n",
        "        plt.setp(axes[i].get_xticklabels(), visible=False)\n",
        "        plt.setp(axes[i].get_yticklabels(), visible=False)\n",
        "        sample = dataset.next()\n",
        "        i += 1\n",
        "    ff.subplots_adjust(wspace=0.1)\n",
        "    plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kGaRa23RfjT",
        "outputId": "e9157e41-55d0-45ce-d17d-1099d8a13ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "gallery(train_ds.shuffle(10).as_numpy_iterator(), num_examples[train_split])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x72 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pgKO2uEU_tn"
      },
      "source": [
        "## Prepare the data for training and testing\n",
        "* We use tensorflow readers; JAX does not have support for input data reading and pre-processing\n",
        "* for training, we use stochastic optimizers (e.g. SGD, Adam), so we need to sample at random mini-batches from the training dataset\n",
        "* for testing, we iterate sequentially through the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZidT8ipGuq9D"
      },
      "source": [
        "### Excercise 2: Permute pixel values\n",
        "In Exercise 2, we evaluate how a convnet generalizes with respect to the spatial permutation of pixel values ![](https://github.com/m2lschool/tutorials2021/blob/main/assets/vision_part1_pixel_permutation.jpg?raw=true).\n",
        "* All images (both training and test images) are modified using the same permuted index list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDTKjPO_4GLG"
      },
      "source": [
        "#@title 28x28 permutation list (for Exercise 3) { form-width: \"40%\" }\n",
        "def get_permutation_mnist():\n",
        "    p = tf.constant([273,  746,  197,  597,  519,  757,  113,  470,\n",
        "                     321,  552,  585,  246,  229,  569,  773,    6,\n",
        "                     379,  548,  148,  503,   27,  132,   82,  101,\n",
        "                     260,   53,  635,  203,  439,  487,  162,  395,\n",
        "                     311,  593,   33,  183,  215,  264,  699,  692,\n",
        "                     560,  124,  126,  708,   41,  368,  484,  467,\n",
        "                     267,  731,   17,   73,   14,  521,  240,  296,\n",
        "                     43,   779,  629,  640,  268,  150,  586,   56,\n",
        "                     756,  769,  382,  354,   95,  283,  775,  432,\n",
        "                     178,  271,  118,  563,  445,  261,  518,  723,\n",
        "                     725,  449,  595,  617,  607,  400,  697,   96,\n",
        "                     656,  138,  343,  653,  175,  433,  681,  654,\n",
        "                     574,  322,  754,  381,   12,  338,  182,  695,\n",
        "                     556,  491,  512,  717,  224,  303,  496,  693,\n",
        "                     140,  599,  332,  758,  465,   80,  501,  690,\n",
        "                     422,  211,  331,  313,  583,  128,  776,  168,\n",
        "                     463,  201,  334,  228,  643,  514,  106,  713,\n",
        "                     507,  543,  703,  299,   74,  263,  710,  622,\n",
        "                     486,  344,  210,  687,  537,  362,  688,  320,\n",
        "                     91,   403,  778,  534,  674,  783,  284,  724,\n",
        "                     549,  184,  605,   15,    8,  535,   85,  495,\n",
        "                     774,  701,  416,  642,  553,  370,  571,  489,\n",
        "                     388,  171,  761,  475,  397,  227,  753,  278,\n",
        "                     155,  748,  745,  437,  414,  181,  759,  234,\n",
        "                     143,  554,  762,   46,  476,  417,  716,  336,\n",
        "                     117,   47,  602,  525,  718,  660,  760,  451,\n",
        "                     142,  609,  405,  455,  315,  394,   36,  389,\n",
        "                     719,  715,  386,  393,  446,  109,  658,  612,\n",
        "                     685,  577,  641,  770,  510,  704,  275,  335,\n",
        "                     259,  307,  435,  712,  326,  232,  237,  172,\n",
        "                     675,   84,  670,  434,  485,   68,  677,  415,\n",
        "                     492,  732,  366,  557,   22,  765,  722,  579,\n",
        "                     288,  308,   48,  198,  481,  604,  139,  647,\n",
        "                     115,  618,  243,  466,  107,  440,  152,  200,\n",
        "                     230,   83,  755,   10,  144,  749,  528,  494,\n",
        "                     199,  546,  282,  223,  346,  352,  421,  763,\n",
        "                     424,  328,  290,   13,   62,  129,  156,  436,\n",
        "                     252,  359,  538,   35,  459,  226,  657,  401,\n",
        "                     191,  483,  187,  242,  680,  646,  473,    4,\n",
        "                     581,  130,  666,  709,    7,  236,  450,  532,\n",
        "                     667,   70,  410,  266,  189,  649,  257,  700,\n",
        "                     500,  188,  634,   25,  517,  573,  104,  387,\n",
        "                     673,  638,  540,  249,  610,  110,  480,  663,\n",
        "                     9,    225,  339,  398,  131,  372,  628,  174,\n",
        "                     488,   79,  766,  310,  468,  691,  425,  289,\n",
        "                     616,  309,  570,  636,  768,  591,  464,  412,\n",
        "                     120,  782,  652,  541,  100,  280,  721,  423,\n",
        "                     430,  442,  506,  160,  502,  333,  615,  399,\n",
        "                     57,   250,  384,   71,  103,  429,  411,   59,\n",
        "                     529,  630,  444,  625,  179,  747,  218,  627,\n",
        "                     408,  443,  305,  733,  509,  274,  682,  584,\n",
        "                     358,  536,  739,  369,  221,  247,  676,  206,\n",
        "                     1,    438,  265,  672,  287,   26,   39,  606,\n",
        "                     479,  102,  291,   88,  205,   61,  127,  351,\n",
        "                     477,  655,  355,   67,   37,  735,  458,  454,\n",
        "                     737,  173,  231,  158,  555,  337,  644,  505,\n",
        "                     233,  730,  431,  530,  580,  312,  720,  441,\n",
        "                     550,  367,  513,   50,  371,   34,   45,  705,\n",
        "                     153,  122,  209,   51,  216,  185,  611,  327,\n",
        "                     603,  428,   42,  669,  601,  620,  771,  116,\n",
        "                     293,  363,   81,   90,  474,   94,  302,   31,\n",
        "                     317,  619,  471,   86,   64,   683,  20,  330,\n",
        "                     472,  650,  714,  380,  196,  272,  736,  349,\n",
        "                      75,  169,   28,  340,  163,  151,  582,  559,\n",
        "                     376,   24,  539,  176,  207,  600,  767,  592,\n",
        "                     726,  277,  511,   98,  498,  668,  298,  292,\n",
        "                     523,  598,  742,  623,  426,  361,  121,  157,\n",
        "                     146,  490,  780,  360,  679,   38,  222,  419,\n",
        "                     192,  587,   30,   77,  702,  235,  318,  751,\n",
        "                     2,    396,  542,  661,  499,   29,   69,  180,\n",
        "                     621,  217,  588,   58,   60,  164,  772,  545,\n",
        "                     452,  170,  752,  281,  478,  711,  648,  575,\n",
        "                     213,  345,   19,  190,  527,  508,  149,  323,\n",
        "                     624,  404,  420,  256,  413,  626,  134,  390,\n",
        "                     614,  342,  565,  238,  241,  781,  590,  533,\n",
        "                     659,  365,  561,  112,  248,  357,  566,  407,\n",
        "                     253,  461,  594,  255,  406,  750,    3,  356,\n",
        "                     141,   97,   92,  522,  734,  325,   54,  738,\n",
        "                     456,  133,  374,   66,  729,  114,  214,  504,\n",
        "                     383,  631,  347,  686,  578,  613,  239,  645,\n",
        "                     764,  427,  651,  568,   87,  119,   63,   65,\n",
        "                     202,  286,  409,  662,  551,   49,  251,  572,\n",
        "                     632,    5,  524,  515,  608,  208,  329,   18,\n",
        "                     516,  350,  295,  448,  385,  678,  258,  204,\n",
        "                     276,  177,   72,  341,   16,  497,  316,  262,\n",
        "                     544,  526,  707,  348,  254,  447,  520,  453,\n",
        "                     270,  304,  558,  462,  418,  279,   99,  353,\n",
        "                     314,  306,  564,  219,  167,  186,  297,  706,\n",
        "                     0,     89,   11,  402,  531,   78,  728,  373,\n",
        "                     562,  684,  194,  195,  294,  567,  698,  378,\n",
        "                     589,   40,  220,  493,  460,   76,  105,  639,\n",
        "                     324,  166,  740,   23,   52,  161,  319,  392,\n",
        "                     135,  111,  391,  547,  145,  123,  744,  364,\n",
        "                     147,  469,  125,  159,  664,   93,  727,  245,\n",
        "                     696,  377,   21,  665,  694,   55,  269,  285,\n",
        "                     671,  165,  193,  244,  457,  741,  375,  482,\n",
        "                     576,  108,  743,  689,  300,   44,   32,  136,\n",
        "                     596,  637,  137,  154,  633,  777,  301,  212])\n",
        "\n",
        "    return p"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUM08vFxuq9G"
      },
      "source": [
        "### Self-supervised learning task: learn the rotation!\n",
        "In exercise 4, we implement a self-supervised approach which consists in randomly rotating the images by multiples of 90 degrees (e.g. 0, 90, 180, or 270 degrees) and learning to recognize the rotation that has been applied. ![](https://github.com/m2lschool/tutorials2021/blob/main/assets/vision_part1_im_rotate.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3MjZSqj0lYt"
      },
      "source": [
        "#@title Dataset loading and preprocessing { form-width: \"40%\" }\n",
        "# We use tensorflow readers; JAX does not have support for input data reading\n",
        "# and pre-processing.\n",
        "def load(ds,\n",
        "         is_training: bool,\n",
        "         flag_permute: bool,\n",
        "         flag_selfsup: bool,\n",
        "         num_transf,\n",
        "         batch_size: int) -> Generator[Batch, None, None]:\n",
        "    \"\"\"Loads the dataset as a generator of batches.\"\"\"\n",
        "    ds = ds.cache().repeat()\n",
        "\n",
        "    if is_training:\n",
        "        ds = ds.shuffle(10 * batch_size, seed=0)\n",
        "\n",
        "    # Define the preprocessing for each train and test image\n",
        "    def preprocess(example):\n",
        "        image, label_transf = _preprocess_image(example['image'],\n",
        "                                                is_training,\n",
        "                                                flag_permute,\n",
        "                                                flag_selfsup,\n",
        "                                                num_transf)\n",
        "        return {'image': image, 'label': example['label'],\n",
        "                'label_transf': label_transf}\n",
        "\n",
        "    # Apply the preprocessing function to all samples in a batch using `map`\n",
        "    ds = ds.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Get samples grouped in mini-batches to train using SGD\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return tfds.as_numpy(ds)  # return numpy array\n",
        "\n",
        "\n",
        "def _preprocess_image(image: tf.Tensor, is_training: bool,\n",
        "                      flag_permute: bool, flag_selfsup: bool,\n",
        "                      num_transf) -> tf.Tensor:\n",
        "    \"\"\"Returns processed and resized image.\"\"\"\n",
        "    # Images are stored as uint8; we convert to float for further processing.\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    # Normalise pixel values between -1 and 1: original images are in [0, 255].\n",
        "    # We normalise to [-1, 1] to have 0 mean and unit variance in the inputs,\n",
        "    # as it makes the training more stable. Note that we do this normalisation\n",
        "    # over the activations of all the layers in the network by using batch\n",
        "    # normalisation layers.\n",
        "    image = 2 * (image / 255.0) - 1.0\n",
        "\n",
        "    # During training, we use data augmentation.\n",
        "    # In this way, we are effectively increasing the size of the training\n",
        "    # dataset, leading to improved generalisation.\n",
        "    label_transf = []\n",
        "    if is_training:\n",
        "\n",
        "        # Data augmentation\n",
        "        # - apply horizontal flip randomly\n",
        "        # - random crop after padding\n",
        "        # - apply optional data augmentation (permutation, rotation)\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "        # Pad images by reflecting the boundaries and randomly sample a\n",
        "        # 32x32 patch.\n",
        "        image = tf.pad(image, [[4, 4], [4, 4], [0, 0]], mode='REFLECT')\n",
        "        image = tf.image.random_crop(image, size=(28, 28, 1))\n",
        "\n",
        "    # Exercise 2: permuted MNIST; scramble the images using a fixed permutation\n",
        "    if flag_permute:\n",
        "        ################\n",
        "        # YOUR CODE HERE  reshape image 2D -> 1D array using tf.reshape\n",
        "        image = tf.reshape(image, 28*28*1)\n",
        "\n",
        "        p = get_permutation_mnist()\n",
        "        ################\n",
        "        # YOUR CODE HERE  permute pixels according to permutation p using\n",
        "        #                 tf.gather(..., axis=0). We have batches \n",
        "        image = tf.gather(params=image, indices=p)\n",
        "        # YOUR CODE HERE  reshape to original shape\n",
        "        image = tf.reshape(image, shape=(28,28,1))\n",
        "\n",
        "    # Excercise 4: data augmentation as self-supervision signal:\n",
        "    # for every image sample uniformly at random a transformation (rotation),\n",
        "    # and apply it to the image while returning the id of the transformation\n",
        "    if is_training and flag_selfsup and num_transf:\n",
        "        ################\n",
        "        # YOUR CODE HERE get a transformation label_transf = tf.random.uniform...\n",
        "        label_transf = tf.random.uniform((num_transf, ))\n",
        "        # YOUR CODE HERE apply transformation image = \\\n",
        "        #                tf.image.rot90(image, k=label_transf)\n",
        "        image = tf.image.rot90(image, k=num_transf)\n",
        "    \n",
        "    return image, label_transf"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m29LP9Tiylp_",
        "outputId": "46ff7a18-4818-44a2-af7e-ea775cebc716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "sample = train_ds.shuffle(1).as_numpy_iterator().next()\n",
        "plt.imshow(sample['image'][:,:,0], cmap='gray')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f404afb00b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5ZtA_mkzEze",
        "outputId": "493eb5f0-d48c-4037-d067-056265fa7682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "img, l = _preprocess_image(image=sample['image'], is_training=True,\n",
        "                      flag_permute=False, flag_selfsup=True, num_transf=4) \n",
        "plt.imshow(img[:,:,0], cmap='gray')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4054814198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q9xIZOJEiiU"
      },
      "source": [
        "## General setting; use the options below to switch between exercises."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b8dDvmpEgwM"
      },
      "source": [
        "MODEL = \"resnet_v2\"  #@param['resnet_v2','mlp']\n",
        "FLAG_UPDATE_BN_STATS = True  #@param['True', 'False'] {type:\"raw\"}\n",
        "FLAG_PERMUTE = False  #@param['True', 'False'] {type:\"raw\"}\n",
        "FLAG_REGULARIZE = False  #@param['True', 'False'] {type:\"raw\"}\n",
        "FLAG_SELFSUP = False  #@param['True', 'False'] {type:\"raw\"}"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OalRJVrVR2up"
      },
      "source": [
        "## Define the model\n",
        "[Squeeze-and-Excitation (SE)](https://arxiv.org/pdf/1709.01507.pdf) blocks to our Resnet18 baseline\n",
        "\n",
        "The Squeeze-and-Excitation blocks (figures from original paper [Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf)) \n",
        "![SE block](https://github.com/eemlcommunity/PracticalSessions2020/blob/master/assets/SE.png?raw=true)\n",
        "implement a simple form of *self-attention*. A self-attention module, in general, receives a number of inputs and produces the same number of outputs. The operations applied inside the block allow the inputs to interact with each other (\"self\") and recalibrate each other, based on which are deemed more important for that particular sample. \n",
        "\n",
        "The goal of SE block is to apply self-attention at the level of feature channels to allow them to recalibrate using more global information (as opposed to the local information available to conv units).    \n",
        "\n",
        "The Squeeze-and-excitation block has two steps:\n",
        "* *squeeze*: Given an input feature block, extract a global descriptor, one value per channel; a simple global descriptor can be obtained through spatial average global pooling\n",
        "* *excite*: Based on the global descriptor, learn a weight vector (using an MLP) to be applied as a gating mechanism on the original features. The output non-linearity of the block is a `sigmoid` (and not e.g. a softmax) to allow multiple feature channels to be emphasised. These weights are then applied (through broadcasting) over the original features, emphasising the features that are more important for those inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4rKFJXE5Qns",
        "outputId": "be0571d4-88d2-4eda-99c3-53e4102bcc82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img.shape"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([28, 28, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTypwslr7jn9",
        "outputId": "ca185332-ac9d-4bbf-edf9-3be2cdd42e24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "jnp.mean(img.numpy(), axis=[1, 2]).shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUmG5uoe5bJU",
        "outputId": "96136909-cc8d-46fb-a1cd-3921553d8d79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "jax.nn.relu"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<jax.custom_derivatives.custom_jvp at 0x7f40c582c438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_LaVpUHj_hW"
      },
      "source": [
        "class SEBlock(hk.Module):\n",
        "    \"\"\"Squeeze and Excitation block.\"\"\"\n",
        "    def __init__(self, channels: Sequence[int],\n",
        "                 name: Optional[str] = None):\n",
        "        super().__init__(name=name)\n",
        "        check_length(2, channels, \"channels\")\n",
        "        self.fc1 = hk.Linear(channels[0])\n",
        "        self.fc2 = hk.Linear(channels[1])\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        ################\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Apply the squeeze step using spatial average global pooling\n",
        "        # YOUR CODE HERE out = jnp.mean(...\n",
        "        out = jnp.mean(inputs, axis=[1, 2])\n",
        "        \n",
        "        # Excite step: linear layer, ReLU, linear layer, sigmoid\n",
        "        #Â YOUR CODE HERE out = ...\n",
        "        out = jax.nn.relu(self.fc1(out))\n",
        "        out = jax.nn.sigmoid(self.fc2(out))\n",
        "        \n",
        "        # Scale the input features with the obtained self-attention weights\n",
        "        # YOUR CODE HERE out = jnp.expand_dims(out, axis=(1, 2))\n",
        "        out = jnp.expand_dims(out, axis=(1, 2))\n",
        "        # YOUR CODE HERE out = ...\n",
        "        out = inputs * out\n",
        "        return out"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkGQo_yfwffs"
      },
      "source": [
        "### SE-Resnet module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjSL6XF6wilp"
      },
      "source": [
        "To apply the SE block to a Resnet architecture, we simply insert the SE block within each resnet block before the element-wise addition between the shortcut and the residual connections\n",
        "\n",
        "![alt text](https://github.com/eemlcommunity/PracticalSessions2020/blob/master/assets/SEresnet.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3Bnix7awvYV"
      },
      "source": [
        "### Create a resnet block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkhQYRJVw0ZN"
      },
      "source": [
        "In a typical sequential model (no branching), the network as a whole is optimised to find the mapping between inputs and correct labels. In residual networks, each layer can learn an additive residual representation wrt to the representation already computed up to the previous layer, making the optimisation easier.\n",
        "\n",
        "As opposed to [resnet-v1](https://arxiv.org/pdf/1512.03385.pdf) blocks (left), [resnet-v2](https://arxiv.org/pdf/1603.05027.pdf) blocks (right) use pre-activation modules, i.e. the batch normalisation (`BN`) and relu (`ReLU`) nonlinearity are applied within the resnet block, before the convolutional layer (`weight`). This allows the model to learn identity mappings over the shortcuts throughout the network, improving further the backpropagation of gradients.   \n",
        "\n",
        "<img src=\"https://github.com/eemlcommunity/PracticalSessions2020/blob/master/assets/v1v2.png?raw=true\" alt=\"resnet blocks\" style=\"width: 80px;\"/>\n",
        "\n",
        "Figure from original [resnet-v2 paper](https://arxiv.org/pdf/1603.05027.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOsLDuGZw4_j"
      },
      "source": [
        "*Bottleneck blocks*: To reduce the number of parameters and memory footprint without sacrificing expressivity, bottleneck blocks can be applied. Instead of using 2 conv layers (`weight` in the figure above) with 3x3 filters, empirically it is shown that projecting in a lower dimensional space (using 1x1 conv layers), applying 3x3 convolutions, and then reprojecting back into the original dimension space, does not affect accuracy.    \n",
        "\n",
        "*1x1 conv shortcuts*: when the input and the output of a resnet block have different numbers of channels, 1x1 convolutional layers are used on the shortcut to project the representation to the desired output feature dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJf0jt34w-I2"
      },
      "source": [
        "## Resnet block with self-attention (coding exercise)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSYh1ejRkDFv"
      },
      "source": [
        "def check_length(length, value, name):\n",
        "    if len(value) != length:\n",
        "        raise ValueError(f\"`{name}` must be of length 4 not {len(value)}\")\n",
        "\n",
        "\n",
        "class BlockV2(hk.Module):\n",
        "    \"\"\"ResNet V2 block with optional bottleneck.\"\"\"\n",
        "    def __init__(self,\n",
        "                 channels: int,\n",
        "                 stride: Union[int, Sequence[int]],\n",
        "                 use_projection: bool,\n",
        "                 bn_config: Mapping[str, float],\n",
        "                 bottleneck: bool,\n",
        "                 name: Optional[str] = None):\n",
        "        super().__init__(name=name)\n",
        "        self.use_projection = use_projection\n",
        "\n",
        "        # Define batch norm parameters: the batch_norm layer normalises the\n",
        "        # inputs to have zero mean and unit variance. To not affect the\n",
        "        # expressivity of the network, e.g. in cases where it would be better\n",
        "        # for the activations to not be 0-centred or to have larger variance,\n",
        "        # batch_norm can optionally learn a scale and an offset parameters.\n",
        "        bn_config = dict(bn_config)\n",
        "        bn_config.setdefault(\"create_scale\", True)\n",
        "        bn_config.setdefault(\"create_offset\", True)\n",
        "\n",
        "        # See comment above about 1x1 conv shortcut\n",
        "        if self.use_projection:\n",
        "            self.proj_conv = hk.Conv2D(\n",
        "                output_channels=channels,\n",
        "                kernel_shape=1,\n",
        "                stride=stride,\n",
        "                with_bias=False,\n",
        "                padding=\"SAME\",\n",
        "                name=\"shortcut_conv\")\n",
        "\n",
        "        # If we use bottleneck blocks (see comment above), inside the resnet\n",
        "        # block we first project the activations into a lower dimensional\n",
        "        # space, which has number of channels divided by `channel_div`\n",
        "        # compared to the desired number of channels in the output.\n",
        "        channel_div = 4 if bottleneck else 1\n",
        "        conv_0 = hk.Conv2D(\n",
        "            output_channels=channels // channel_div,\n",
        "            kernel_shape=1 if bottleneck else 3,\n",
        "            stride=1,\n",
        "            with_bias=False,\n",
        "            padding=\"SAME\",\n",
        "            name=\"conv_0\")\n",
        "\n",
        "        bn_0 = hk.BatchNorm(name=\"batchnorm_0\", **bn_config)\n",
        "        # Then we apply the 3x3 conv layer\n",
        "        conv_1 = hk.Conv2D(\n",
        "            output_channels=channels // channel_div,\n",
        "            kernel_shape=3,\n",
        "            stride=stride,\n",
        "            with_bias=False,\n",
        "            padding=\"SAME\",\n",
        "            name=\"conv_1\")\n",
        "\n",
        "        bn_1 = hk.BatchNorm(name=\"batchnorm_1\", **bn_config)\n",
        "        layers = ((conv_0, bn_0), (conv_1, bn_1))\n",
        "\n",
        "        # When using bottleneck, we have also a 3rd 1x1 convolutional layer\n",
        "        # within the resnet block (see comment above about bottleneck blocks)\n",
        "        if bottleneck:\n",
        "            conv_2 = hk.Conv2D(\n",
        "                output_channels=channels,\n",
        "                kernel_shape=1,\n",
        "                stride=1,\n",
        "                with_bias=False,\n",
        "                padding=\"SAME\",\n",
        "                name=\"conv_2\")\n",
        "\n",
        "            bn_2 = hk.BatchNorm(name=\"batchnorm_2\", **bn_config)\n",
        "            layers = layers + ((conv_2, bn_2),)\n",
        "\n",
        "        self.layers = layers\n",
        "        ################\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Define the self-attention block\n",
        "        # YOUR CODE HERE self.se_block = ...\n",
        "\n",
        "    def __call__(self, inputs, is_training, test_local_stats):\n",
        "        x = shortcut = inputs\n",
        "        for i, (conv_i, bn_i) in enumerate(self.layers):\n",
        "            # Apply pre-activation: batch_norm + relu\n",
        "            x = bn_i(x, is_training, test_local_stats)\n",
        "            x = jax.nn.relu(x)\n",
        "            # If using 1x1 conv projection on the shortcut,\n",
        "            # apply proj_conv once\n",
        "            if i == 0 and self.use_projection:\n",
        "                shortcut = self.proj_conv(x)\n",
        "            # Apply convolution\n",
        "            x = conv_i(x)\n",
        "\n",
        "        ################\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Apply the SE block before the element-wise addition between\n",
        "        # the shortcut and the residual\n",
        "        # YOUR CODE HERE x = ...\n",
        "\n",
        "        return x + shortcut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceXoZkkvkJKL"
      },
      "source": [
        "#@title Stack resnet blocks { form-width: \"40%\" }\n",
        "class BlockGroup(hk.Module):\n",
        "    \"\"\"Group of blocks for ResNet implementation.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        num_blocks: int,\n",
        "        stride: Union[int, Sequence[int]],\n",
        "        bn_config: Mapping[str, float],\n",
        "        bottleneck: bool,\n",
        "        use_projection: bool,\n",
        "        name: Optional[str] = None):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.blocks = []\n",
        "        for i in range(num_blocks):\n",
        "            self.blocks.append(\n",
        "                BlockV2(channels=channels,\n",
        "                        stride=(1 if i else stride),\n",
        "                        use_projection=(i == 0 and use_projection),\n",
        "                        bottleneck=bottleneck,\n",
        "                        bn_config=bn_config,\n",
        "                        name=\"block_%d\" % (i)))\n",
        "\n",
        "    def __call__(self, inputs, is_training, test_local_stats):\n",
        "        out = inputs\n",
        "        for block in self.blocks:\n",
        "            out = block(out, is_training, test_local_stats)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKw0Kk5QkNCD"
      },
      "source": [
        "#@title Define a generic resnet architecture { form-width: \"40%\" }\n",
        "# Note: This class is generic, it can be used to instantiate any Resnet\n",
        "# model, e.g. Resnet-50, Resnet-101, etc. by substituting the correct block\n",
        "# parameters\n",
        "class ResNet(hk.Module):\n",
        "    \"\"\"ResNet model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        blocks_per_group: Sequence[int],\n",
        "        num_classes: int,\n",
        "        num_transf: str,\n",
        "        bn_config: Optional[Mapping[str, float]] = None,\n",
        "        bottleneck: bool = True,\n",
        "        channels_per_group: Sequence[int] = (256, 512, 1024, 2048),\n",
        "        use_projection: Sequence[bool] = (True, True, True, True),\n",
        "        name: Optional[str] = None,\n",
        "    ):\n",
        "        \"\"\"Constructs a ResNet model.\n",
        "        Args:\n",
        "            blocks_per_group: A sequence of length 4 that indicates the number\n",
        "                of blocks created in each group.\n",
        "            num_classes: The number of classes to classify the inputs into.\n",
        "            bn_config: A dictionary of two elements, `decay_rate` and `eps` to\n",
        "                be passed on to the `BatchNorm` layers. By default the\n",
        "                `decay_rate` is `0.9` and `eps` is `1e-5`.\n",
        "            bottleneck: Whether the block should bottleneck or not. Defaults\n",
        "                to True.\n",
        "            channels_per_group: A sequence of length 4 that indicates the\n",
        "                number of channels used for each block in each group.\n",
        "            use_projection: A sequence of length 4 that indicates whether each\n",
        "                residual block should use projection.\n",
        "            name: Name of the module.\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "        bn_config = dict(bn_config or {})\n",
        "        bn_config.setdefault(\"decay_rate\", 0.9)\n",
        "        bn_config.setdefault(\"eps\", 1e-5)\n",
        "        bn_config.setdefault(\"create_scale\", True)\n",
        "        bn_config.setdefault(\"create_offset\", True)\n",
        "\n",
        "        # Number of blocks in each group for ResNet.\n",
        "        check_length(4, blocks_per_group, \"blocks_per_group\")\n",
        "        check_length(4, channels_per_group, \"channels_per_group\")\n",
        "\n",
        "        # We first convolve the image with 7x7 filters, to be able to better\n",
        "        # extract low-level features such as contours. Using conv with stride=2\n",
        "        # halves the resolution of the input, reducing considerably the\n",
        "        # computation cost, and increasing the receptive field.\n",
        "        self.initial_conv = hk.Conv2D(\n",
        "            output_channels=64,\n",
        "            kernel_shape=7,\n",
        "            stride=2,\n",
        "            with_bias=False,\n",
        "            padding=\"SAME\",\n",
        "            name=\"initial_conv\")\n",
        "\n",
        "        self.block_groups = []\n",
        "        strides = (1, 2, 2, 2)\n",
        "        for i in range(4):\n",
        "            self.block_groups.append(\n",
        "                BlockGroup(channels=channels_per_group[i],\n",
        "                           num_blocks=blocks_per_group[i],\n",
        "                           stride=strides[i],\n",
        "                           bn_config=bn_config,\n",
        "                           bottleneck=bottleneck,\n",
        "                           use_projection=use_projection[i],\n",
        "                           name=\"block_group_%d\" % (i)))\n",
        "\n",
        "        self.final_batchnorm = hk.BatchNorm(name=\"final_batchnorm\", **bn_config)\n",
        "\n",
        "        # Add output classifier\n",
        "        self.logits = hk.Linear(num_classes, w_init=jnp.zeros, name=\"logits\")\n",
        "\n",
        "        # Add second head for transformation prediction\n",
        "        self.logits_transf = None\n",
        "        if num_transf:\n",
        "            self.logits_transf = hk.Linear(num_transf, w_init=jnp.zeros,\n",
        "                                           name=\"logits_transf\")\n",
        "\n",
        "    def __call__(self, inputs, is_training, test_local_stats=False):\n",
        "        out = inputs\n",
        "        out = self.initial_conv(out)\n",
        "        # Reduce the spatial resolution of the activations by a factor of 2.\n",
        "        # This increases the receptive field and reduces the computation cost.\n",
        "        # Note that compared to a strided conv which has the same effects,\n",
        "        # the pooling layers does not have trainable parameters.\n",
        "        out = hk.max_pool(out,\n",
        "                          window_shape=(1, 3, 3, 1),\n",
        "                          strides=(1, 2, 2, 1),\n",
        "                          padding=\"SAME\")\n",
        "\n",
        "        for block_group in self.block_groups:\n",
        "            out = block_group(out, is_training, test_local_stats)\n",
        "\n",
        "        out = self.final_batchnorm(out, is_training, test_local_stats)\n",
        "        out = jax.nn.relu(out)\n",
        "\n",
        "        # Pool over spatial dimensions to obtain the final vector embedding\n",
        "        # of the image. Use jnp.mean and not hk.avg_pool, to make sure that the\n",
        "        # network can be applied to inputs with any resolution without\n",
        "        # modification of the model.\n",
        "        out = jnp.mean(out, axis=[1, 2])\n",
        "\n",
        "        logits_transf = None\n",
        "        if self.logits_transf:\n",
        "            logits_transf = self.logits_transf(out)\n",
        "\n",
        "        return self.logits(out), logits_transf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbGAIjXUkSbw"
      },
      "source": [
        "#@title Instantiate Resnet18 { form-width: \"40%\" }\n",
        "class ResNet18(ResNet):\n",
        "    \"\"\"ResNet18.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes: int, num_transf: str,\n",
        "                 bn_config: Optional[Mapping[str, float]] = None,\n",
        "                 name: Optional[str] = None):\n",
        "        \"\"\"Constructs a ResNet model.\n",
        "        Args:\n",
        "          num_classes: The number of classes to classify the inputs into.\n",
        "          bn_config: A dictionary of two elements, `decay_rate` and `eps` to be\n",
        "              passed on to the `BatchNorm` layers.\n",
        "          name: Name of the module.\n",
        "        \"\"\"\n",
        "        super().__init__(blocks_per_group=(2, 2, 2, 2),\n",
        "                         num_classes=num_classes,\n",
        "                         num_transf=num_transf,\n",
        "                         bn_config=bn_config,\n",
        "                         bottleneck=False,\n",
        "                         channels_per_group=(64, 128, 256, 512),\n",
        "                         use_projection=(False, True, True, True),\n",
        "                         name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gcsY1Mjaf_s"
      },
      "source": [
        "## Define simple MLP baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYekdfZBaiTy"
      },
      "source": [
        "class MLP(hk.Module):\n",
        "    def __init__(self, num_classes=10, num_transf=None):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_transf = num_transf\n",
        "\n",
        "    def __call__(self, inputs, is_training=True):\n",
        "        batch_norm_args = {\n",
        "            \"is_training\": is_training\n",
        "        }\n",
        "        bs = inputs.shape[0]\n",
        "        inputs = jnp.reshape(inputs, [bs, -1])\n",
        "        net = hk.Linear(1024)(inputs)\n",
        "        net = jax.nn.relu(net)\n",
        "        net = hk.BatchNorm(True,\n",
        "                           True,\n",
        "                           0.99,\n",
        "                           eps=0.001,\n",
        "                           name=\"bn_postnorm1\")(inputs, **batch_norm_args)\n",
        "        net = hk.Linear(1024)(net)\n",
        "        net = jax.nn.relu(net)\n",
        "        net = hk.BatchNorm(True,\n",
        "                           True,\n",
        "                           0.99,\n",
        "                           eps=0.001,\n",
        "                           name=\"bn_postnorm2\")(net, **batch_norm_args)\n",
        "        logits = hk.Linear(self.num_classes, name=\"logits\")(net)\n",
        "        logits_transf = None\n",
        "        if self.num_transf:\n",
        "            logits_transf = hk.Linear(self.num_transf,\n",
        "                                      name=\"logits_transf\")(net)\n",
        "        return logits, logits_transf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPex0rz3auId"
      },
      "source": [
        "## Hyper-parameters for training and optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoTsOIRQSPV0"
      },
      "source": [
        "INIT_RANDOM_SEED = 42  #@param\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "TEST_BATCH_SIZE = 100\n",
        "\n",
        "# Model parameters\n",
        "# NUM_TRANSF can be None or 4 corresponding to 4 rotations (0, 90, 180, 270)\n",
        "NUM_TRANSF = 4  #@param\n",
        "\n",
        "# reduction ratio for SE blocks\n",
        "REDUCTION_RATIO = 16  #@param\n",
        "\n",
        "# Optimizer parameters\n",
        "REGLOSS_WEIGHT_DECAY = 1e-4  #@param\n",
        "OPTIMIZER_MOMENTUM = 0.9  #@param\n",
        "OPTIMIZER_USE_NESTEROV = True  #@param\n",
        "\n",
        "# Define number of training iterations and reporting intervals\n",
        "TRAIN_ITERS = 100e2  #@param\n",
        "SAVE_MODEL_EVERY = 100  #@param\n",
        "REPORT_TEST_EVERY = 1000  #@param\n",
        "TEST_ITERS = (num_examples[eval_split]) // TEST_BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "117ebPugarCO"
      },
      "source": [
        "### Instanciate either MLP baseline or convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfHEpqp3DZbR"
      },
      "source": [
        "if MODEL == 'mlp':\n",
        "    net_fn = lambda *args: MLP(num_classes=num_classes,\n",
        "                               num_transf=NUM_TRANSF)(*args)\n",
        "  \n",
        "else:  # model is resnet_v2\n",
        "    net_fn = lambda *args: ResNet18(num_classes, NUM_TRANSF)(*args)\n",
        "\n",
        "# We use transform with state because we need to keep the state of the network,\n",
        "# e.g. for batch norm statistics.\n",
        "net = hk.transform_with_state(net_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-F4W5niV1sm"
      },
      "source": [
        "# Get number of parameters in a scope by iterating through the\n",
        "# trainable variables\n",
        "def get_num_params(params: hk.Params) -> jnp.ndarray:\n",
        "    total_parameters = 0\n",
        "    for p in jax.tree_leaves(params):\n",
        "        total_parameters += jnp.prod(jnp.asarray(p.shape))\n",
        "    return total_parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aUrDkAASwLY"
      },
      "source": [
        "#@title Define learning rate schedule and optimizer { form-width: \"40%\" }\n",
        "# We use learning rate annealing during training. We start with a larger\n",
        "# learning rate `lr_init` which allows exploring faster the space of solutions\n",
        "# and we reduce it by a factor of 10 `lr_factor` after a predefined number of\n",
        "# steps. Smaller learning rate at the end of the training allows the model to\n",
        "# explore a local neighbourhood and settle on a good local minimum.\n",
        "def lr_schedule(step: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Define learning rate annealing schedule.\"\"\"\n",
        "    # After how many steps to apply the learning rate reduction\n",
        "    boundaries = jnp.array((5000, 8000, 9500))  # after how many iterations\n",
        "    # to reduce the learning rate\n",
        "    # Every time we hit a predefined number of steps, we apply the reduction\n",
        "    # of the learning rate by `lr_factor`\n",
        "    lr_decay_exponent = jnp.sum(step >= boundaries)\n",
        "    lr_init = 0.1  # initial value for learning rate\n",
        "    lr_factor = 0.1  # reduce learning rate by this factor\n",
        "    return lr_init * lr_factor**lr_decay_exponent\n",
        "\n",
        "\n",
        "# Define the optimiser, we use SGD with nesterov momentum\n",
        "def make_optimizer():\n",
        "    \"\"\"SGD and a custom lr schedule.\"\"\"\n",
        "    return optax.chain(optax.trace(decay=OPTIMIZER_MOMENTUM,\n",
        "                                   nesterov=OPTIMIZER_USE_NESTEROV),\n",
        "                       optax.scale_by_schedule(lr_schedule),\n",
        "                       optax.scale(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSh0bFEGSgJH"
      },
      "source": [
        "#@title Define the loss function: cross-entropy for classification and weight decay for regularization { form-width: \"40%\" }\n",
        "# Function to compute l2 loss - useful for regularisation\n",
        "def l2_loss(params: Iterable[jnp.ndarray]) -> jnp.ndarray:\n",
        "    return 0.5 * sum(jnp.sum(jnp.square(p)) for p in params)\n",
        "\n",
        "\n",
        "# Function to compute softmax cross entropy for classification\n",
        "def softmax_cross_entropy(\n",
        "    *,\n",
        "    logits: jnp.ndarray,\n",
        "    labels: jnp.ndarray,\n",
        ") -> jnp.ndarray:\n",
        "    return -jnp.sum(labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "\n",
        "\n",
        "def loss_fn(\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        "    batch: Batch\n",
        ") -> Tuple[jnp.ndarray, hk.State]:\n",
        "    \"\"\"Computes a regularized loss for the given batch.\"\"\"\n",
        "    # The third parameter would be an rng key if one is needed in running\n",
        "    # the model, e.g. for dropout. If not needed, pass `None`.\n",
        "    (logits, logits_selfsup), state = net.apply(params, state, None,\n",
        "                                                batch['image'], True)\n",
        "\n",
        "    # Estimate accuracy for the current batch\n",
        "    predicted_label = jnp.argmax(logits, axis=-1)\n",
        "    correct = jnp.sum(jnp.equal(predicted_label, batch['label']))\n",
        "    acc = correct.astype(jnp.float32) / TRAIN_BATCH_SIZE\n",
        "\n",
        "    # The labels are given as class indices; convert to one_hot representation\n",
        "    labels = jax.nn.one_hot(batch['label'], num_classes)\n",
        "    # Compute classification loss\n",
        "    loss = jnp.mean(softmax_cross_entropy(logits=logits, labels=labels))\n",
        "\n",
        "    # Exercise 3 - Add regularization\n",
        "    if FLAG_REGULARIZE:\n",
        "        ################\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Get all the trainable parameters of the model, except batch_norm\n",
        "        # parameters to apply weight decay regularisation , i.e. we penalise\n",
        "        # weights with large magnitude\n",
        "        # YOUR CODE HERE l2_params = ...\n",
        "        l2_params = [p for ((mod_name, _), p) in tree.flatten_with_path(params)\n",
        "                     if 'batchnorm' not in mod_name]\n",
        "        # We apply a weighting factor to the regularisation loss, so that it\n",
        "        # does not dominate the total loss\n",
        "        # YOUR CODE HERE reg_loss = REGLOSS_WEIGHT_DECAY * ...\n",
        "        # YOUR CODE HERE loss += ...\n",
        "        \n",
        "    # Exercise 4 - Add auxiliary loss for self-supervised learning\n",
        "    if FLAG_SELFSUP:\n",
        "        ################\n",
        "        # YOUR CODE HERE labels_selfsup = jax.nn.one_hot(batch['label_transf'], ...\n",
        "        # YOUR CODE HERE loss += ...\n",
        "\n",
        "    return loss, (state, acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hi1vyVhDEyO"
      },
      "source": [
        "#@title Define the training step { form-width: \"40%\" }\n",
        "@jax.jit\n",
        "def train_step(\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        "    opt_state: OptState,\n",
        "    batch: Batch,\n",
        ") -> Tuple[hk.Params, hk.State, OptState, jnp.ndarray]:\n",
        "    \"\"\"Applies an update to parameters and returns new state.\"\"\"\n",
        "    (loss, aux_data), grads = (\n",
        "      jax.value_and_grad(loss_fn, has_aux=True)(params, state, batch))\n",
        "\n",
        "    state, acc = aux_data\n",
        "\n",
        "    # Compute and apply updates via our optimizer.\n",
        "    updates, opt_state = make_optimizer().update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    return params, state, opt_state, loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-RfHRb1DGO0"
      },
      "source": [
        "#@title Define the evaluation { form-width: \"40%\" }\n",
        "@jax.jit\n",
        "def eval_batch(\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        "    batch: Batch,\n",
        ") -> jnp.ndarray:\n",
        "    \"\"\"Evaluates a batch.\"\"\"\n",
        "    # The third parameter would be an rng key if one is needed in running\n",
        "    # the model, e.g. for dropout. If not needed, pass `None`.\n",
        "    (logits, _), state = net.apply(params, state, None, batch['image'], False)\n",
        "    predicted_label = jnp.argmax(logits, axis=-1)\n",
        "    correct = jnp.sum(jnp.equal(predicted_label, batch['label']))\n",
        "    return correct.astype(jnp.float32)\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        ") -> jnp.ndarray:\n",
        "    \"\"\"Evaluates the model at the given params/state.\"\"\"\n",
        "    correct = jnp.array(0)\n",
        "    total = 0\n",
        "    test_dataset = load(test_ds, is_training=False, batch_size=TEST_BATCH_SIZE,\n",
        "                        flag_permute=FLAG_PERMUTE, flag_selfsup=False,\n",
        "                        num_transf=None)\n",
        "    test_dataset = iter(test_dataset)\n",
        "    for eval_iter in range(TEST_ITERS):\n",
        "        correct += eval_batch(params, state, next(test_dataset))\n",
        "        total += TEST_BATCH_SIZE\n",
        "\n",
        "    return correct.item() / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_dTKQblSzkm"
      },
      "source": [
        "#@title Initialise the model and the optimiser { form-width: \"40%\" }\n",
        "def make_initial_state(\n",
        "    rng: jnp.ndarray,\n",
        "    batch: Batch,\n",
        ") -> Tuple[hk.Params, hk.State, OptState]:\n",
        "    \"\"\"Computes the initial network state.\"\"\"\n",
        "    params, state = net.init(rng, batch['image'], True)\n",
        "    opt_state = make_optimizer().init(params)\n",
        "    return params, state, opt_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ARGcbSGTFDN"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc0wJi1Uw_8V"
      },
      "source": [
        "#@title Show training statistics using Tensorboard { form-width: \"40%\" }\n",
        "import datetime\n",
        "exp_dir = \"exp_\" + MODEL + (\"\", \"_regularize\")[FLAG_REGULARIZE] + \\\n",
        "    (\"\", \"_permutepixels\")[FLAG_PERMUTE] + (\"\", \"_selfsup\")[FLAG_SELFSUP] + \\\n",
        "    (\"\", \"_updatebn\")[FLAG_UPDATE_BN_STATS] + \"/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  exp_dir + \"log/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JySfLZEM3f5O"
      },
      "source": [
        "# Launch the viewer\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {\".\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76mgn8H3FHG2"
      },
      "source": [
        "# Get training dataset\n",
        "train_dataset = load(train_ds, is_training=True, batch_size=TRAIN_BATCH_SIZE,\n",
        "                     flag_permute=FLAG_PERMUTE, flag_selfsup=FLAG_SELFSUP,\n",
        "                     num_transf=NUM_TRANSF)\n",
        "train_dataset = iter(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3bkVvF0FD8P"
      },
      "source": [
        "rng = jax.random.PRNGKey(INIT_RANDOM_SEED)\n",
        "\n",
        "# Initialization requires an example input to calculate shapes of parameters.\n",
        "batch = next(train_dataset)\n",
        "params, state, opt_state = make_initial_state(rng, batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7VKCh9ySddK"
      },
      "source": [
        "# Get number of parameters in the model.\n",
        "print(\"Total number of parameters of models\")\n",
        "print(get_num_params(params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f3rHQ2HpF5w"
      },
      "source": [
        "for step_num in range(int(TRAIN_ITERS)):\n",
        "    # Take a training step.\n",
        "    batch = next(train_dataset)\n",
        "    params, state, opt_state, train_loss, train_acc = \\\n",
        "    train_step(params, state, opt_state, batch)\n",
        "\n",
        "    # We run evaluation during training to see the progress.\n",
        "    if REPORT_TEST_EVERY > 0 and step_num % REPORT_TEST_EVERY == 0:\n",
        "        eval_acc = evaluate(params, state)\n",
        "\n",
        "    # Save model at fixed intervals.\n",
        "    if step_num % SAVE_MODEL_EVERY == 0:\n",
        "        with open(os.path.join(exp_dir, 'params.pkl'), 'wb') as handle:\n",
        "            pickle.dump(params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        with open(os.path.join(exp_dir, 'state.pkl'), 'wb') as handle:\n",
        "            pickle.dump(state, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    with summary_writer.as_default():\n",
        "        tf.summary.scalar('train_loss', train_loss, step=step_num)\n",
        "        tf.summary.scalar('train_acc', train_acc, step=step_num)\n",
        "        tf.summary.scalar('test_acc', eval_acc, step=step_num)\n",
        "\n",
        "# Once training has finished we run eval one more time to get final results.\n",
        "eval_acc = evaluate(params, state)\n",
        "print('[Eval acc FINAL]: %s' % (eval_acc))\n",
        "\n",
        "# Save last model parameters\n",
        "with open(os.path.join(exp_dir, 'params.pkl'), 'wb') as handle:\n",
        "    pickle.dump(params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(os.path.join(exp_dir, 'state.pkl'), 'wb') as handle:\n",
        "    pickle.dump(state, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PSKFpn358cI"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNRIlI-96Hcv"
      },
      "source": [
        "!ls {exp_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQsnFyW46nN6"
      },
      "source": [
        "# restoring the latest checkpoint in exp_dir\n",
        "with open(os.path.join(exp_dir, 'params.pkl'), 'rb') as handle:\n",
        "    params = pickle.load(handle)\n",
        "\n",
        "with open(os.path.join(exp_dir, 'state.pkl'), 'rb') as handle:\n",
        "    state = pickle.load(handle)\n",
        "\n",
        "eval_acc = evaluate(params, state)\n",
        "print('[Eval acc]: %s' % (eval_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}